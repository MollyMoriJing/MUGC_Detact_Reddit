{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "import contractions\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from textstat import flesch_reading_ease, syllable_count\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>human_answers</th>\n",
       "      <th>chatgpt_answers</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Why is every book I hear about a \" NY Times # ...</td>\n",
       "      <td>[Basically there are many categories of \" Best...</td>\n",
       "      <td>[There are many different best seller lists th...</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>If salt is so bad for cars , why do we use it ...</td>\n",
       "      <td>[salt is good for not dying in car crashes and...</td>\n",
       "      <td>[Salt is used on roads to help melt ice and sn...</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why do we still have SD TV channels when HD lo...</td>\n",
       "      <td>[The way it works is that old TV stations got ...</td>\n",
       "      <td>[There are a few reasons why we still have SD ...</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Why has nobody assassinated Kim Jong - un He i...</td>\n",
       "      <td>[You ca n't just go around assassinating the l...</td>\n",
       "      <td>[It is generally not acceptable or ethical to ...</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How was airplane technology able to advance so...</td>\n",
       "      <td>[Wanting to kill the shit out of Germans drive...</td>\n",
       "      <td>[After the Wright Brothers made the first powe...</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                           question  \\\n",
       "0  0  Why is every book I hear about a \" NY Times # ...   \n",
       "1  1  If salt is so bad for cars , why do we use it ...   \n",
       "2  2  Why do we still have SD TV channels when HD lo...   \n",
       "3  3  Why has nobody assassinated Kim Jong - un He i...   \n",
       "4  4  How was airplane technology able to advance so...   \n",
       "\n",
       "                                       human_answers  \\\n",
       "0  [Basically there are many categories of \" Best...   \n",
       "1  [salt is good for not dying in car crashes and...   \n",
       "2  [The way it works is that old TV stations got ...   \n",
       "3  [You ca n't just go around assassinating the l...   \n",
       "4  [Wanting to kill the shit out of Germans drive...   \n",
       "\n",
       "                                     chatgpt_answers       source  \n",
       "0  [There are many different best seller lists th...  reddit_eli5  \n",
       "1  [Salt is used on roads to help melt ice and sn...  reddit_eli5  \n",
       "2  [There are a few reasons why we still have SD ...  reddit_eli5  \n",
       "3  [It is generally not acceptable or ethical to ...  reddit_eli5  \n",
       "4  [After the Wright Brothers made the first powe...  reddit_eli5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the dataset from Hello-SimpleAI/HC3\n",
    "dataset = load_dataset(\"Hello-SimpleAI/HC3\", \"all\")\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "\n",
    "#take a look at the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10802</th>\n",
       "      <td>[Photons are smaller than the air molecules . ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18118</th>\n",
       "      <td>[Discs using the DVD-Video specification requi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17763</th>\n",
       "      <td>[Over one million Jewish children were killed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37572</th>\n",
       "      <td>[Your voice may sound deeper in the morning be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47333</th>\n",
       "      <td>[It can be helpful to hire a certified public ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  classification\n",
       "10802  [Photons are smaller than the air molecules . ...               1\n",
       "18118  [Discs using the DVD-Video specification requi...               1\n",
       "17763  [Over one million Jewish children were killed ...               1\n",
       "37572  [Your voice may sound deeper in the morning be...               0\n",
       "47333  [It can be helpful to hire a certified public ...               0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine the human answers and chatgpt answers into one column\n",
    "df = df.melt(value_vars=[\"human_answers\", \"chatgpt_answers\"], var_name=\"original_column\", value_name=\"text\")\n",
    "\n",
    "#classify the human answers as 1 and the chatgpt answers as 0\n",
    "df['classification'] = df['original_column'].apply(lambda x: 1 if x == 'human_answers' else 0)\n",
    "\n",
    "#drop the original column\n",
    "df = df.drop(columns=['original_column']).reset_index(drop=True)\n",
    "\n",
    "# Use only a sample for debugging\n",
    "df = df.sample(500)\n",
    "\n",
    "#take a look at the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the summary of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 500 entries, 10802 to 36854\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   text            500 non-null    object\n",
      " 1   classification  500 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 11.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  classification  \\\n",
      "10802  Photons are smaller than the air molecules . T...               1   \n",
      "18118  Discs using the DVD-Video specification requir...               1   \n",
      "17763  Over one million Jewish children were killed i...               1   \n",
      "37572  Your voice may sound deeper in the morning bec...               0   \n",
      "47333  It can be helpful to hire a certified public a...               0   \n",
      "\n",
      "       word_count  \n",
      "10802         689  \n",
      "18118         168  \n",
      "17763         139  \n",
      "37572         856  \n",
      "47333        1295  \n",
      "Total number of words in the text column: 658163\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to strings\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x)) if isinstance(x, str) else 0)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "total_words = df['word_count'].sum()\n",
    "\n",
    "print(f\"Total number of words in the text column: {total_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra whitespaces\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()) if isinstance(x, str) else x)\n",
    "\n",
    "# Normalize Unicode characters\n",
    "df['text'] = df['text'].apply(lambda x: unicodedata.normalize('NFKD', x) if isinstance(x, str) else x)\n",
    "\n",
    "# Expand contractions (e.g., \"don't\" → \"do not\")\n",
    "df['text'] = df['text'].apply(lambda x: contractions.fix(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Remove digits and special characters (keep punctuation)\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^a-zA-Z.,!?; ]+', '', x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "# Remove stopwords and apply lemmatization\n",
    "stop_words = set(stopwords.words('english')) # set the language \n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())  # Convert to lowercase here\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and token.text.lower() not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "    \n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: preprocess_text(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word count\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()) if isinstance(x, str) else 0)\n",
    "\n",
    "# Extract sentence count\n",
    "df[\"sentence_count\"] = df[\"text\"].apply(lambda x: len(nltk.sent_tokenize(x)) if isinstance(x, str) else 0)\n",
    "\n",
    "# Compute average sentence length\n",
    "df[\"avg_sentence_length\"] = df[\"word_count\"] / (df[\"sentence_count\"] + 1)  # Avoid division by zero\n",
    "\n",
    "# Compute punctuation count\n",
    "df[\"punctuation_count\"] = df[\"text\"].apply(lambda x: len(re.findall(r'[.,!?;]', x)) if isinstance(x, str) else 0)\n",
    "\n",
    "# Compute readability score\n",
    "df[\"readability_score\"] = df[\"text\"].apply(lambda x: flesch_reading_ease(x) if isinstance(x, str) else 0)\n",
    "\n",
    "# Compute unique word ratio\n",
    "df[\"unique_word_ratio\"] = df[\"text\"].apply(lambda x: len(set(x.split())) / len(x.split()) if isinstance(x, str) and len(x.split()) > 0 else 0)\n",
    "\n",
    "# Compute text complexity metrics\n",
    "df[\"avg_word_length\"] = df[\"text\"].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if isinstance(x, str) and len(x.split()) > 0 else 0)\n",
    "df[\"avg_syllables_per_word\"] = df[\"text\"].apply(lambda x: syllable_count(x) / len(x.split()) if isinstance(x, str) and len(x.split()) > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract n-grams (bigrams & trigrams)\n",
    "def get_ngram_frequencies(text, n=2):\n",
    "    words = text.split()\n",
    "    ngrams = zip(*[words[i:] for i in range(n)])\n",
    "    return Counter(ngrams).most_common(10)\n",
    "\n",
    "df[\"bigrams\"] = df[\"text\"].apply(lambda x: get_ngram_frequencies(x, 2))\n",
    "df[\"trigrams\"] = df[\"text\"].apply(lambda x: get_ngram_frequencies(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute POS (Parts of Speech) tagging\n",
    "def get_pos_tags(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "    return pos_counts\n",
    "\n",
    "df[\"pos_tags\"] = df[\"text\"].apply(lambda x: get_pos_tags(x) if isinstance(x, str) else {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentiment analysis\n",
    "df[\"sentiment_polarity\"] = df[\"text\"].apply(lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else 0)\n",
    "df[\"sentiment_subjectivity\"] = df[\"text\"].apply(lambda x: TextBlob(x).sentiment.subjectivity if isinstance(x, str) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_features = vectorizer.fit_transform(df[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  classification  \\\n",
      "10802  photon small air molecule thus fly fast sound ...               1   \n",
      "18118  disc use dvdvideo specification require dvd dr...               1   \n",
      "17763  one million jewish child kill holocaust approx...               1   \n",
      "37572  voice may sound deeply morning use much night ...               0   \n",
      "47333  helpful hire certify public accountant cpa han...               0   \n",
      "\n",
      "       word_count  sentence_count  avg_sentence_length  punctuation_count  \\\n",
      "10802          56               1                 28.0                  0   \n",
      "18118          17               1                  8.5                  0   \n",
      "17763          15               1                  7.5                  0   \n",
      "37572          69               1                 34.5                  0   \n",
      "47333         105               1                 52.5                  0   \n",
      "\n",
      "       readability_score  unique_word_ratio  avg_word_length  \\\n",
      "10802              -2.29           0.678571         5.857143   \n",
      "18118              -5.01           0.705882         5.647059   \n",
      "17763              30.87           0.733333         5.933333   \n",
      "37572               1.44           0.637681         5.463768   \n",
      "47333             -60.49           0.619048         6.028571   \n",
      "\n",
      "       avg_syllables_per_word  \\\n",
      "10802                1.785714   \n",
      "18118                2.294118   \n",
      "17763                1.866667   \n",
      "37572                1.550725   \n",
      "47333                1.942857   \n",
      "\n",
      "                                                 bigrams  \\\n",
      "10802  [((sonic, boom), 2), ((act, like), 2), ((photo...   \n",
      "18118  [((dvd, drive), 2), ((dvd, player), 2), ((disc...   \n",
      "17763  [((million, jewish), 3), ((one, million), 1), ...   \n",
      "37572  [((become, flexible), 2), ((sound, high), 2), ...   \n",
      "47333  [((business, taxis), 3), ((cpa, handle), 2), (...   \n",
      "\n",
      "                                                trigrams  \\\n",
      "10802  [((photon, small, air), 1), ((small, air, mole...   \n",
      "18118  [((disc, use, dvdvideo), 1), ((use, dvdvideo, ...   \n",
      "17763  [((one, million, jewish), 1), ((million, jewis...   \n",
      "37572  [((voice, may, sound), 1), ((may, sound, deepl...   \n",
      "47333  [((ensure, tax, return), 2), ((helpful, hire, ...   \n",
      "\n",
      "                                                pos_tags  sentiment_polarity  \\\n",
      "10802  {'PROPN': 6, 'ADJ': 7, 'NOUN': 25, 'ADV': 3, '...            0.275000   \n",
      "18118                 {'NOUN': 8, 'VERB': 1, 'PROPN': 8}            0.000000   \n",
      "17763  {'NUM': 6, 'ADJ': 3, 'NOUN': 4, 'VERB': 1, 'AD...           -0.100000   \n",
      "37572  {'NOUN': 33, 'AUX': 1, 'VERB': 12, 'ADV': 5, '...            0.250583   \n",
      "47333  {'ADJ': 14, 'NOUN': 62, 'VERB': 21, 'ADV': 4, ...            0.092370   \n",
      "\n",
      "       sentiment_subjectivity  \n",
      "10802                0.516667  \n",
      "18118                0.000000  \n",
      "17763                0.150000  \n",
      "37572                0.425250  \n",
      "47333                0.520400  \n",
      "Total words in dataset: 57132\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(f\"Total words in dataset: {df['word_count'].sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
